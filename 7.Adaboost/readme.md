# 理论部分
boosting各分类器之间是并行训练的，它是关注被已有的分类器错分的那些数据来获得新的分类器，并且对每个分类器分配不同的权重，
每个权重代表的是该分类器对这一轮分类的成功度。其中最经典最流行的boosting集成方法是AdaBoost（adaptive boosting）。
主要过程为：1.给数据样本权重2.训练数据样本，得到分类器3.算每个分类器的错误率，给分类器分权重4.将分类器分错的样本权重值增加，分错的样本权重减小
5.用新样本训练数据，一个新的分类器再从第三步重新进行6.知道步骤三中分类器的错误为零，或者到达迭代次数就终止7.将所有弱分类器加权求和得到分类结果

# 代码部分
Adaboost代码，主要包括四部分：
一、构建单层决策树，仅基于单个特征来做决策。
本文选择了五个简单的数据，但是这五个数据简单的分类器一下子不能将他们分隔开，所以需要用到这章节讲到的集成分类器，来提高简单分类器的性能。 二、单层决策树生成函数
1.选择合适的分类方式
2.分类训练函数。第一个循环为了切割所有的特征值X1和X2，第二个循环是找到其中的一个特征值，改变步长对这一个特征值进行划分，第三个循环是对阈值的划分取一种情况看哪个好，阈值划分有两种，一种是左边为-1，右边为+1，另一种是左边为+1，右边为-1，看最后两种分类方式哪个好。
三、基于单层决策树的Adaboost训练过程
给出了完整的计算过程。
四、Adaboost分类函数
将弱分类器的训练过程从程序中抽取出来，然后应用到某个具体实例中去。
dataMatrix：数据矩阵； dimen：第n特征； threshVal：阈值； threshIneq：标志不等号
numit：迭代次数；D：样本权重；a：分类器权重
